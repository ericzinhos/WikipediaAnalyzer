{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVvc24qXA8FfP3Eqc9+pgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericzinhos/WikipediaAnalyzer/blob/main/WikipediaAnalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MÓDULO I – Análise de mudança de vieses ao longo do tempo\n"
      ],
      "metadata": {
        "id": "Euexas9oTixX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exWUT_pQTffn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from datetime import datetime\n",
        "import time\n",
        "import string\n",
        "\n",
        "# Baixa as stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Funções\n",
        "def formatar_data(timestamp):\n",
        "    return datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "def coletar_versoes(titulo_pagina):\n",
        "    API_URL = \"https://pt.wikipedia.org/w/api.php\"\n",
        "    pasta_saida = f\"versoes_{titulo_pagina.replace(' ', '_')}\"\n",
        "    os.makedirs(pasta_saida, exist_ok=True)\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"revisions\",\n",
        "        \"titles\": titulo_pagina,\n",
        "        \"rvprop\": \"timestamp|content\",\n",
        "        \"rvlimit\": \"500\",\n",
        "        \"rvdir\": \"newer\",\n",
        "        \"continue\": \"\"\n",
        "    }\n",
        "\n",
        "    contador_versoes = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(f\"Fazendo requisição... (Versões coletadas até agora: {contador_versoes})\")\n",
        "            response = requests.get(API_URL, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            pages = data[\"query\"][\"pages\"]\n",
        "            for page in pages:\n",
        "                revisions = pages[page][\"revisions\"]\n",
        "                for revision in revisions:\n",
        "                    contador_versoes += 1\n",
        "                    timestamp = revision[\"timestamp\"]\n",
        "\n",
        "                    if \"*\" in revision:\n",
        "                        content = revision[\"*\"]\n",
        "                    else:\n",
        "                        print(f\"Conteúdo não encontrado na versão {contador_versoes}. Pulando...\")\n",
        "                        continue\n",
        "\n",
        "                    numero_versao = f\"{contador_versoes:04d}\"\n",
        "                    data_formatada = formatar_data(timestamp)\n",
        "                    nome_arquivo = f\"{pasta_saida}/versao_{numero_versao}_{data_formatada}.txt\"\n",
        "\n",
        "                    with open(nome_arquivo, \"w\", encoding=\"utf-8\") as arquivo:\n",
        "                        arquivo.write(content)\n",
        "\n",
        "            if \"continue\" in data:\n",
        "                params.update(data[\"continue\"])\n",
        "            else:\n",
        "                print(\"Todas as edições foram coletadas.\")\n",
        "                break\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro na requisição: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Todas as {contador_versoes} versões foram salvas na pasta '{pasta_saida}'.\")\n",
        "    return pasta_saida\n",
        "\n",
        "def grafico_total_palavras(pasta_saida, titulo_pagina):\n",
        "    total_palavras_por_versao = []\n",
        "\n",
        "    for nome_arquivo in sorted(os.listdir(pasta_saida)):\n",
        "        if nome_arquivo.endswith(\".txt\"):\n",
        "            caminho_arquivo = os.path.join(pasta_saida, nome_arquivo)\n",
        "            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
        "                texto = arquivo.read()\n",
        "                total_palavras = len(texto.split())\n",
        "                total_palavras_por_versao.append(total_palavras)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(total_palavras_por_versao, label='Total de Palavras')\n",
        "    plt.xlabel('Versões')\n",
        "    plt.ylabel('Número de Palavras')\n",
        "    plt.title(f'Número Total de Palavras por Versão ({titulo_pagina})')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def mostrar_grafico_com_menu():\n",
        "    plt.show()\n",
        "    plt.pause(0.001)  # Pequena pausa para atualizar a figura\n",
        "\n",
        "def grafico_frequencia_palavras(pasta_saida, titulo_pagina):\n",
        "    contagens_ao_longo_do_tempo = []\n",
        "\n",
        "    for nome_arquivo in sorted(os.listdir(pasta_saida)):\n",
        "        if nome_arquivo.endswith(\".txt\"):\n",
        "            caminho_arquivo = os.path.join(pasta_saida, nome_arquivo)\n",
        "            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
        "                texto = arquivo.read()\n",
        "                contagem = processar_texto(texto)\n",
        "                contagens_ao_longo_do_tempo.append(contagem)\n",
        "\n",
        "    palavras_usuario = input(\"Digite uma ou mais palavras (separadas por vírgula): \").strip().lower()\n",
        "    palavras = [palavra.strip() for palavra in palavras_usuario.split(',')]\n",
        "\n",
        "    frequencias = {palavra: [] for palavra in palavras}\n",
        "    for contagem in contagens_ao_longo_do_tempo:\n",
        "        for palavra in palavras:\n",
        "            frequencias[palavra].append(contagem.get(palavra, 0))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for palavra in palavras:\n",
        "        plt.plot(frequencias[palavra], label=palavra)\n",
        "\n",
        "    plt.xlabel('Versões')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.title(f'Frequência das Palavras ao Longo do Tempo ({titulo_pagina})')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def grafico_tfidf(pasta_saida, titulo_pagina):\n",
        "    textos = []\n",
        "\n",
        "    for nome_arquivo in sorted(os.listdir(pasta_saida)):\n",
        "        if nome_arquivo.endswith(\".txt\"):\n",
        "            caminho_arquivo = os.path.join(pasta_saida, nome_arquivo)\n",
        "            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
        "                texto = arquivo.read()\n",
        "                texto = texto.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "                textos.append(texto)\n",
        "\n",
        "    palavras_usuario = input(\"Digite uma ou mais palavras (separadas por vírgula): \").strip().lower()\n",
        "    palavras = [palavra.strip() for palavra in palavras_usuario.split(',')]\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        vocabulary=palavras,\n",
        "        lowercase=True,\n",
        "        stop_words=list(stop_words))\n",
        "\n",
        "    tfidf = vectorizer.fit_transform(textos)\n",
        "    scores_tfidf = tfidf.toarray()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i, palavra in enumerate(palavras):\n",
        "        plt.plot(scores_tfidf[:, i], label=palavra)\n",
        "\n",
        "    plt.xlabel('Versões')\n",
        "    plt.ylabel('Score TF-IDF')\n",
        "    plt.title(f'Evolução do TF-IDF das Palavras ao Longo do Tempo ({titulo_pagina})')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def grafico_palavras_mais_frequentes(pasta_saida, titulo_pagina, n=10):\n",
        "    todas_palavras = Counter()\n",
        "\n",
        "    for nome_arquivo in sorted(os.listdir(pasta_saida)):\n",
        "        if nome_arquivo.endswith(\".txt\"):\n",
        "            caminho_arquivo = os.path.join(pasta_saida, nome_arquivo)\n",
        "            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
        "                texto = arquivo.read()\n",
        "                contagem = processar_texto(texto)\n",
        "                todas_palavras += contagem\n",
        "\n",
        "    palavras_mais_comuns = todas_palavras.most_common(n)\n",
        "    palavras, frequencias = zip(*palavras_mais_comuns)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(palavras, frequencias, color='skyblue')\n",
        "    plt.xlabel('Palavras')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.title(f'As {n} Palavras Mais Frequentes ({titulo_pagina})')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Processamento de texto e contagem de palavras\n",
        "def processar_texto(texto):\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    palavras = texto.split()\n",
        "    palavras = [palavra for palavra in palavras if palavra not in stop_words and len(palavra) > 2]\n",
        "    return Counter(palavras)\n",
        "\n",
        "# Apaga os dados\n",
        "def apagar_dados(pasta_saida):\n",
        "    if os.path.exists(pasta_saida):\n",
        "        shutil.rmtree(pasta_saida)\n",
        "        print(f\"Pasta '{pasta_saida}' removida.\")\n",
        "    else:\n",
        "        print(f\"Pasta '{pasta_saida}' não encontrada.\")\n",
        "\n",
        "# Menu interativo\n",
        "def menu_principal():\n",
        "    while True:\n",
        "        # Solicita o título da página\n",
        "        titulo_pagina = input(\"Digite o título da página da Wikipédia: \").strip()\n",
        "\n",
        "        # Coleta as versões do verbete\n",
        "        pasta_saida = coletar_versoes(titulo_pagina)\n",
        "\n",
        "        # Menu de opções após a coleta\n",
        "        while True:\n",
        "            print(\"\\nEscolha uma opção:\")\n",
        "            print(\"1. Gerar gráfico de total de palavras por versão\")\n",
        "            print(\"2. Gerar gráfico de frequência de palavras específicas\")\n",
        "            print(\"3. Gerar gráfico de TF-IDF\")\n",
        "            print(\"4. Gerar gráfico das palavras mais frequentes\")\n",
        "            print(\"5. Escolher novo verbete (mantém dados atuais)\")\n",
        "            print(\"6. Apagar dados e sair\")\n",
        "\n",
        "            opcao = input(\"Opção: \").strip()\n",
        "\n",
        "            if opcao == \"1\":\n",
        "                grafico_total_palavras(pasta_saida, titulo_pagina)\n",
        "            elif opcao == \"2\":\n",
        "                grafico_frequencia_palavras(pasta_saida, titulo_pagina)\n",
        "            elif opcao == \"3\":\n",
        "                grafico_tfidf(pasta_saida, titulo_pagina)\n",
        "            elif opcao == \"4\":\n",
        "                grafico_palavras_mais_frequentes(pasta_saida, titulo_pagina)\n",
        "            elif opcao == \"5\":\n",
        "                break  # Volta ao início para escolher um novo verbete\n",
        "            elif opcao == \"6\":\n",
        "                confirmacao = input(\"Tem certeza que deseja apagar todos os dados e sair? (s/n): \").strip().lower()\n",
        "                if confirmacao == 's':\n",
        "                    # Apaga todas as pastas que começam com 'versoes_'\n",
        "                    for pasta in os.listdir():\n",
        "                        if pasta.startswith('versoes_'):\n",
        "                            shutil.rmtree(pasta)\n",
        "                            print(f\"Pasta '{pasta}' removida.\")\n",
        "                print(\"Saindo...\")\n",
        "                return  # Encerra o programa\n",
        "            else:\n",
        "                print(\"Opção inválida. Tente novamente.\")\n",
        "\n",
        "# Executa o menu principal\n",
        "menu_principal()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MÓDULO II – Análise de redes de usuários"
      ],
      "metadata": {
        "id": "hScBM2WNTlgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mwclient plotly pandas networkx tqdm\n",
        "\n",
        "import mwclient\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "class WikipediaVisualizer:\n",
        "    def __init__(self):\n",
        "        self.site = mwclient.Site('pt.wikipedia.org', clients_useragent='TCC_Visualizador/4.0')\n",
        "        self.edits = []\n",
        "        self.G = nx.DiGraph()\n",
        "\n",
        "    def fetch_all_edits(self, article_title):\n",
        "        self.article_title = article_title\n",
        "        page = self.site.pages[article_title]\n",
        "        self.edits = list(tqdm(\n",
        "            page.revisions(prop='user|comment|tags|ids|roles|timestamp', dir='newer', limit=None),\n",
        "            desc='Coletando edições',\n",
        "            unit=' edições'\n",
        "        ))\n",
        "\n",
        "    def _get_user_type(self, username):\n",
        "        try:\n",
        "            user_page = self.site.pages[f'Usuário:{username}']\n",
        "            text = user_page.text()\n",
        "            if any(tag in text for tag in ['Administrador/Topo', 'Wikipedia:Userbox/Administrador']):\n",
        "                return 'Admin'\n",
        "            if any(tag in text for tag in ['{{Wikipedia-Bots}}', '[[Categoria:!Robôs]]']):\n",
        "                return 'Bot'\n",
        "            if 'autorrevisor' in text.lower():\n",
        "                return 'Autorrevisor'\n",
        "            return 'Editor'\n",
        "        except Exception as e:\n",
        "            return 'Não classificado'\n",
        "\n",
        "    def _is_revert(self, current, previous):\n",
        "        has_revert_tag = 'tags' in current and 'mw-revert' in current['tags']\n",
        "        has_parent_match = ('parentid' in current and 'revid' in previous and\n",
        "                          current['parentid'] == previous['revid'])\n",
        "        comment = current.get('comment', '').lower()\n",
        "        revert_keywords = ['Etiqueta: Desfazer', 'Etiquetas: Desfazer', 'Etiquetas: Reversão', 'Etiqueta: Reversão', 'Reversão manual', 'Etiquetas: Reversão e Avisos', 'Desfeita a edição', 'Desfeita(s) uma ou mais edições']\n",
        "        has_revert_comment = any(kw in comment for kw in revert_keywords)\n",
        "        has_edit_id = any(word.isdigit() and len(word) >= 1 for word in comment.split())\n",
        "        criteria = [has_revert_tag, has_parent_match, has_revert_comment, has_edit_id]\n",
        "        return (sum(criteria) >= 1) and (has_revert_comment or has_edit_id)\n",
        "\n",
        "    def build_network(self):\n",
        "        user_data = defaultdict(lambda: {'edits': 0, 'type': None})\n",
        "        self.reverts = []\n",
        "        self.collaborations = []\n",
        "\n",
        "        for rev in self.edits:\n",
        "            if 'user' in rev and rev['user'] != 'Anonymous':\n",
        "                user = rev['user']\n",
        "                user_data[user]['edits'] += 1\n",
        "                if not user_data[user]['type']:\n",
        "                    user_data[user]['type'] = self._get_user_type(user)\n",
        "\n",
        "        for i in tqdm(range(1, len(self.edits)), desc='Analisando relações'):\n",
        "            current = self.edits[i]\n",
        "            previous = self.edits[i-1]\n",
        "\n",
        "            if not all('user' in r for r in [current, previous]):\n",
        "                continue\n",
        "\n",
        "            u1, u2 = current['user'], previous['user']\n",
        "            if u1 == u2 or 'Anonymous' in [u1, u2]:\n",
        "                continue\n",
        "\n",
        "            self.collaborations.append((u1, u2))\n",
        "            if self._is_revert(current, previous):\n",
        "                self.reverts.append((u1, u2))\n",
        "\n",
        "        for user, data in user_data.items():\n",
        "            self.G.add_node(user, **data)\n",
        "\n",
        "        self.G.add_edges_from(\n",
        "            [(u, v, {'type': 'colab', 'color': '#1f77b4'}) for u, v in self.collaborations]\n",
        "        )\n",
        "        self.G.add_edges_from(\n",
        "            [(u, v, {'type': 'revert', 'color': '#d62728'}) for u, v in self.reverts]\n",
        "        )\n",
        "\n",
        "    def visualize(self):\n",
        "        pos = nx.spring_layout(self.G, k=0.7, seed=42, iterations=100)\n",
        "\n",
        "        node_colors = {\n",
        "            'Admin': '#9467bd',\n",
        "            'Bot': '#ff7f0e',\n",
        "            'Autorrevisor': '#1f77b4',\n",
        "            'Editor': '#2ca02c',\n",
        "            'Não classificado': '#7f7f7f'\n",
        "        }\n",
        "\n",
        "        node_traces = []\n",
        "        for group, color in node_colors.items():\n",
        "            nodes = [n for n, d in self.G.nodes(data=True) if d['type'] == group]\n",
        "            sizes = [15 + math.log(d['edits']+1)*10 for n, d in self.G.nodes(data=True) if d['type'] == group]\n",
        "\n",
        "            node_traces.append(go.Scatter(\n",
        "                x=[pos[n][0] for n in nodes],\n",
        "                y=[pos[n][1] for n in nodes],\n",
        "                mode='markers+text',\n",
        "                text=[n if self.G.nodes[n]['edits'] > 5 else '' for n in nodes],\n",
        "                marker=dict(size=sizes, color=color),\n",
        "                name=group,\n",
        "                hoverinfo='text',\n",
        "                hovertext=[f\"Usuário: {n}<br>Edições: {d['edits']}\" for n, d in self.G.nodes(data=True) if d['type'] == group]\n",
        "            ))\n",
        "\n",
        "        edge_traces = []\n",
        "        for edge_type, color, name in [('colab', '#1f77b4', 'Colaborações'), ('revert', '#d62728', 'Reversões')]:\n",
        "            edges = [(u, v) for u, v, d in self.G.edges(data=True) if d['type'] == edge_type]\n",
        "            x_edges = []\n",
        "            y_edges = []\n",
        "            for u, v in edges:\n",
        "                x_edges += [pos[u][0], pos[v][0], None]\n",
        "                y_edges += [pos[u][1], pos[v][1], None]\n",
        "\n",
        "            edge_traces.append(go.Scatter(\n",
        "                x=x_edges,\n",
        "                y=y_edges,\n",
        "                line=dict(width=1, color=color),\n",
        "                mode='lines',\n",
        "                name=name\n",
        "            ))\n",
        "\n",
        "        stats_text = (\n",
        "            f\"Edições totais: {len(self.edits)}<br>\"\n",
        "            f\"Editores únicos: {self.G.number_of_nodes()}<br>\"\n",
        "            f\"Reversões detectadas: {len(self.reverts)}\"\n",
        "        )\n",
        "\n",
        "        layout = go.Layout(\n",
        "            title=f'<b>{self.article_title}</b><br><sub>Análise de Rede</sub>',\n",
        "            showlegend=True,\n",
        "            legend=dict(x=1.05, y=1, title='Legenda:'),\n",
        "            margin=dict(b=150),\n",
        "            annotations=[dict(\n",
        "                text=stats_text,\n",
        "                showarrow=False,\n",
        "                x=0.05,\n",
        "                y=-0.3,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                align='left',\n",
        "                bgcolor='white',\n",
        "                bordercolor='black'\n",
        "            )],\n",
        "            xaxis=dict(showgrid=False, zeroline=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False)\n",
        "        )\n",
        "\n",
        "        fig = go.Figure(data=edge_traces + node_traces, layout=layout)\n",
        "        fig.show()\n",
        "\n",
        "# Fluxo principal de execução\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = WikipediaVisualizer()\n",
        "    article_title = input(\"Digite o título do artigo da Wikipedia a ser analisado: \").strip()\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nIniciando análise do artigo: {article_title}\")\n",
        "        analyzer.fetch_all_edits(article_title)\n",
        "        analyzer.build_network()\n",
        "        analyzer.visualize()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nErro durante a análise: {str(e)}\")\n",
        "        print(\"Verifique se o título está correto e tente novamente.\")"
      ],
      "metadata": {
        "id": "qwXAY6s6Tn_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MÓDULO III – Integração com Voyant Tools"
      ],
      "metadata": {
        "id": "tnGWfUcfTp6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coleta do texto das versões em .txt (zipado) para upload no Voyant Tools"
      ],
      "metadata": {
        "id": "MJQsMB1ATs0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "from urllib.parse import unquote\n",
        "import time\n",
        "\n",
        "# Função para limpar o conteúdo wiki\n",
        "def limpar_conteudo(texto):\n",
        "    # Remover marcações específicas\n",
        "    padroes = [\n",
        "        r'\\[\\[(?:[^|]*?\\|)??([^]]*?)\\]\\]',     # Links wiki\n",
        "        r'\\{\\{.*?\\}\\}',                        # Templates\n",
        "        r'<ref.*?>.*?</ref>',                  # Referências\n",
        "        r'<[^>]+>',                            # Tags HTML\n",
        "        r'\\|\\s*width=.*?px',                   # Atributos de tabela\n",
        "        r'\\[http[^ ]+ (.*?)\\]',                # URLs externas\n",
        "        r'\\{\\|.*?\\|\\}',                        # Tabelas\n",
        "        r'==\\s*Ver também\\s*==.*',             # Seções específicas\n",
        "        r'==\\s*Referências\\s*==.*',            # Seção de referências\n",
        "        r'\\'\\'\\'',                             # Marcações de ênfase\n",
        "        r'\\s+',                                # Espaços múltiplos\n",
        "    ]\n",
        "\n",
        "    for padrao in padroes:\n",
        "        texto = re.sub(padrao, ' ', texto, flags=re.DOTALL)\n",
        "\n",
        "    return texto.strip()\n",
        "\n",
        "# Função principal\n",
        "def gerar_zip_versoes():\n",
        "    # Configurações do usuário\n",
        "    lang = input(\"Digite o código do idioma (ex: 'pt'): \").strip()\n",
        "    article_title = input(\"Digite o título do verbete: \").strip()\n",
        "    article_title_clean = unquote(article_title).replace(' ', '_')\n",
        "\n",
        "    # Configurar API\n",
        "    API_URL = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"revisions\",\n",
        "        \"titles\": article_title,\n",
        "        \"rvprop\": \"timestamp|content\",\n",
        "        \"rvlimit\": \"max\",\n",
        "        \"rvdir\": \"newer\",\n",
        "        \"continue\": \"\"\n",
        "    }\n",
        "\n",
        "    # Criar ZIP\n",
        "    with zipfile.ZipFile(f'{article_title_clean}_versoes.zip', 'w') as zipf:\n",
        "        contador = 0\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = requests.get(API_URL, params=params, timeout=30)\n",
        "                data = response.json()\n",
        "                pages = data[\"query\"][\"pages\"]\n",
        "\n",
        "                for page in pages:\n",
        "                    revisions = pages[page].get(\"revisions\", [])\n",
        "\n",
        "                    for rev in revisions:\n",
        "                        contador += 1\n",
        "                        content = rev.get('*', '')\n",
        "\n",
        "                        # Limpar conteúdo\n",
        "                        content_clean = limpar_conteudo(content)\n",
        "\n",
        "                        # Formatar nome do arquivo\n",
        "                        timestamp = rev[\"timestamp\"].replace(':', '-').replace('T', '_')\n",
        "                        filename = f\"{article_title_clean}_{timestamp}_v{contador:04d}.txt\"\n",
        "\n",
        "                        # Adicionar ao ZIP\n",
        "                        zipf.writestr(filename, content_clean)\n",
        "\n",
        "                        print(f\"Versão {contador} processada: {filename}\")\n",
        "\n",
        "                if \"continue\" in data:\n",
        "                    params.update(data[\"continue\"])\n",
        "                    time.sleep(1)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro: {str(e)}\")\n",
        "                break\n",
        "\n",
        "    print(f\"\\n✅ {contador} versões salvas em {article_title_clean}_versoes.zip\")\n",
        "\n",
        "# Executar\n",
        "if __name__ == \"__main__\":\n",
        "    gerar_zip_versoes()"
      ],
      "metadata": {
        "id": "tje6xJ2FTuw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coleta dos links das versões"
      ],
      "metadata": {
        "id": "NN2QBotfTwq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.parse import quote\n",
        "from datetime import datetime\n",
        "\n",
        "# Solicitar entradas do usuário\n",
        "lang = input(\"Digite o código do idioma da Wikipédia (ex: 'pt', 'en'): \").strip()\n",
        "article_title = input(\"Digite o título do verbete: \").strip()\n",
        "\n",
        "# Perguntar ao usuário quais informações adicionais deseja\n",
        "show_numbers = input(\"Mostrar lista numerada? (s/n): \").strip().lower() == 's'\n",
        "show_timestamps = input(\"Mostrar data e hora de publicação? (s/n): \").strip().lower() == 's'\n",
        "\n",
        "# Configurar API\n",
        "api_url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
        "params = {\n",
        "    \"action\": \"query\",\n",
        "    \"prop\": \"revisions\",\n",
        "    \"titles\": article_title,\n",
        "    \"rvprop\": \"ids|timestamp\",\n",
        "    \"rvlimit\": \"max\",\n",
        "    \"rvdir\": \"newer\",\n",
        "    \"format\": \"json\"\n",
        "}\n",
        "\n",
        "revisions_data = []\n",
        "max_revisions = 1000\n",
        "\n",
        "try:\n",
        "    count = 0\n",
        "    while count < max_revisions:\n",
        "        response = requests.get(api_url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if \"error\" in data:\n",
        "            print(f\"Erro: {data['error']['info']}\")\n",
        "            break\n",
        "\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        if \"missing\" in page:\n",
        "            print(\"Página não encontrada!\")\n",
        "            break\n",
        "\n",
        "        revisions = page.get(\"revisions\", [])\n",
        "        revisions_data.extend([\n",
        "            {\n",
        "                \"revid\": rev[\"revid\"],\n",
        "                \"timestamp\": rev[\"timestamp\"] if \"timestamp\" in rev else None\n",
        "            } for rev in revisions\n",
        "        ])\n",
        "        count += len(revisions)\n",
        "\n",
        "        if \"continue\" in data and count < max_revisions:\n",
        "            params.update(data[\"continue\"])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "# Processar título para URL\n",
        "safe_title = quote(article_title.replace(\" \", \"_\"), safe=\"/:\")\n",
        "\n",
        "# Gerar saída formatada\n",
        "if revisions_data:\n",
        "    print(f\"\\nPrimeiras {len(revisions_data)} versões históricas:\")\n",
        "    for i, rev in enumerate(revisions_data[:max_revisions], 1):\n",
        "        # Formatar linha de saída\n",
        "        line_parts = []\n",
        "\n",
        "        if show_numbers:\n",
        "            line_parts.append(f\"{i}.\")\n",
        "\n",
        "        line_parts.append(f\"https://{lang}.wikipedia.org/w/index.php?title={safe_title}&oldid={rev['revid']}\")\n",
        "\n",
        "        if show_timestamps and rev['timestamp']:\n",
        "            try:\n",
        "                # Converter timestamp ISO para formato mais legível\n",
        "                dt = datetime.strptime(rev['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                formatted_time = dt.strftime(\"%d/%m/%Y %H:%M\")\n",
        "                line_parts.append(f\"(publicada em {formatted_time})\")\n",
        "            except ValueError:\n",
        "                line_parts.append(f\"(timestamp: {rev['timestamp']})\")\n",
        "\n",
        "        print(\" \".join(line_parts))\n",
        "else:\n",
        "    print(\"Nenhuma versão histórica encontrada.\")"
      ],
      "metadata": {
        "id": "0b1jrjUnTycC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}